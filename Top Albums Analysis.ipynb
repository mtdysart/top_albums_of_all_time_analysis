{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import requests\n",
    "import time\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from bs4 import BeautifulSoup\n",
    "from urllib.parse import urljoin\n",
    "from selenium import webdriver\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scraping the data from the Top Albums of All Time\n",
    "\n",
    "### Sputnikmusic\n",
    "We need to get the content from the top albums of all time list. To do this, we first use Selenium webdriver to select 'all time' from the dropdown list. Then we can extract the content into a BeautifulSoup object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[WDM] - Current google-chrome version is 85.0.4183\n",
      "[WDM] - Get LATEST driver version for 85.0.4183\n",
      "[WDM] - Driver [C:\\Users\\mattd\\.wdm\\drivers\\chromedriver\\win32\\85.0.4183.87\\chromedriver.exe] found in cache\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " \n"
     ]
    }
   ],
   "source": [
    "def get_sput_alltime():\n",
    "    \"\"\"\n",
    "    Returns a Beautiful Soup object containing the contents of the Top Albums of All Time page from sputnikmusic.com.\n",
    "    Returns None if content fails to load.\n",
    "    \"\"\"\n",
    "    \n",
    "    current_year = \"2020\"\n",
    "    rankings_url = \"https://www.sputnikmusic.com/best/albums/\" + current_year + \"/\"\n",
    "\n",
    "    driver = webdriver.Chrome(ChromeDriverManager().install())\n",
    "    driver.maximize_window()\n",
    "    driver.get(rankings_url)\n",
    "\n",
    "    time.sleep(5)\n",
    "    content = driver.page_source.encode('utf-8').strip()\n",
    "\n",
    "    if content is not None:\n",
    "        soup = BeautifulSoup(content, 'html.parser')\n",
    "\n",
    "        # Find the correct id for the dropdown\n",
    "        yr_dd = soup.find(\"span\", text=current_year)\n",
    "        dd_id = yr_dd.parent.parent.get('id')\n",
    "\n",
    "        # Select 'All Time' from the dropdown\n",
    "        driver.find_element_by_id(dd_id).click()\n",
    "        driver.find_element_by_id(dd_id + \"_o_2\").click()\n",
    "\n",
    "        # Wait for webpage to load and then create the parse tree from the HTML\n",
    "        time.sleep(5)\n",
    "        content_all_time = driver.page_source.encode('utf-8').strip()\n",
    "\n",
    "        if content_all_time is not None:\n",
    "            soup_all_time = BeautifulSoup(content_all_time, 'html.parser')\n",
    "            driver.quit()\n",
    "            \n",
    "            return soup_all_time\n",
    "\n",
    "\n",
    "    driver.quit()\n",
    "    return None\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We create a pandas DataFrame object from the data obtained above and store it in a variable called data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Rank                Artist                                          Album  \\\n",
      "0      1            Pink Floyd                             Wish You Were Here   \n",
      "1      2  Ludwig van Beethoven             Symphony No. 9 in D minor, Op. 125   \n",
      "2      3        Charles Mingus            The Black Saint and the Sinner Lady   \n",
      "3      4                   Nas                                       Illmatic   \n",
      "4      5           Miles Davis                                   Kind of Blue   \n",
      "..   ...                   ...                                            ...   \n",
      "195  196           Suffocation                            Pierced from Within   \n",
      "196  197         Leonard Cohen                         Songs of Leonard Cohen   \n",
      "197  198            Koji Kondo  The Legend of Zelda 25th Anniversary Symphony   \n",
      "198  199             Sigur Ros                                 AgÃ¦tis byrjun   \n",
      "199  200                Slayer                                 Reign in Blood   \n",
      "\n",
      "    Score Ratings  \n",
      "0    4.64    6635  \n",
      "1    4.63     469  \n",
      "2     4.6    1030  \n",
      "3     4.6    3537  \n",
      "4     4.6    2394  \n",
      "..    ...     ...  \n",
      "195  4.38    1200  \n",
      "196  4.37     631  \n",
      "197  4.37     158  \n",
      "198  4.37    2621  \n",
      "199  4.37    4797  \n",
      "\n",
      "[200 rows x 5 columns]\n"
     ]
    }
   ],
   "source": [
    "def create_data_frame(soup):\n",
    "    \"\"\"\n",
    "    Creates the main data frame for the all-time list.\n",
    "    \"\"\"\n",
    "    \n",
    "    table = soup.find(\"tr\", class_=\"alt1\").parent.parent\n",
    "    cells = table.find_all(\"td\", class_=\"blackbox\")\n",
    "    \n",
    "    common_str = \" votes\"\n",
    "    data = []\n",
    "    for i in range(0, len(cells), 2):\n",
    "        row = {}\n",
    "        row['Rank'] = str(int(cells[i].text)) # Remove leading 0's\n",
    "        \n",
    "        fonts = cells[i+1].find_all(\"font\")\n",
    "        row['Artist'] = fonts[0].text \n",
    "        row['Album'] = fonts[1].text\n",
    "        row['Score'] = fonts[3].text\n",
    "        row['Ratings'] = fonts[4].text[: -len(common_str)]\n",
    "        \n",
    "        data.append(row)\n",
    "    \n",
    "    return pd.DataFrame(data)\n",
    "    \n",
    "data = create_data_frame(soup_all_time)\n",
    "print(data)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unfortunately, the data on this page does not include the release year or genre(s) of each album, so we need to scrape those from their corresponding album and artist pages respectively. This needs to be done carefully so as to not overload the site with many requests per second."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_album_links(soup):\n",
    "    \"\"\"\n",
    "    Returns the album page links for each album in sputnik all time list.\n",
    "    \"\"\"\n",
    "    \n",
    "    base_url = \"https://www.sputnikmusic.com\"\n",
    "    table = soup.find(\"tr\", class_=\"alt1\").parent.parent\n",
    "    \n",
    "    links = [urljoin(base_url, a.get('href')) for a in table.find_all(\"a\")]\n",
    "    return links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200\n"
     ]
    }
   ],
   "source": [
    "links = get_album_links(soup_all_time)\n",
    "print(len(links))\n",
    "\n",
    "r = requests.get(links[0])\n",
    "if r.status_code == 200:\n",
    "    soup = BeautifulSoup(r.content, \"html.parser\")\n",
    "    \n",
    "    with open(\"album_page.html\", \"wb\") as file:\n",
    "        file.write(soup.prettify(\"utf-8\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
